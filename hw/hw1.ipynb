{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1 (38 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1 (13 pts)\n",
    "\n",
    "- (1 pts) Докажите, что $\\| A \\|_F \\le \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$.\n",
    "- (1 pts) Покажите, что для любых $m, n$ и $k \\le \\min(m, n)$ существует $A \\in \\mathbb{R}^{m \\times n}: \\mathrm{rank}(A) = k$, такая что $\\| A \\|_F = \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$.\n",
    "- (1 pts) Докажите, что $\\| A B \\|_F \\le \\| A \\|_2 \\| B \\|_F$.\n",
    "- (1 pts) Докажите, что $\\|A\\|_2 \\leq \\sqrt{\\|A\\|_1 \\|A\\|_{\\infty}}$ \n",
    "- (1 pts) Докажите субмультипликативность Фробениусовой нормы\n",
    "- (2 pts) Пусть матрица $A \\in \\mathbb{C}^{n \\times n}$ и её сингулярное разложение $A = U\\Sigma V^*$. Найдите сингулярное разложение матрицы $\\begin{bmatrix} 0 & A^* \\\\ A & 0 \\end{bmatrix}$ размера $2n \\times 2n$\n",
    "- (2 pts) Докажите, что $\\|UAV\\|_2 = \\| A\\|_2$ и $\\|UAV\\|_F = \\| A\\|_F$ для квадратных унитарных матриц $U$ и $V$, при этом матрица $A$ не обязательно квадратная.\n",
    "- (2 pts) Докажите, что если матрица треугольная и ортогональная, то она диагональная.\n",
    "- (2 pts) Пусть $S$ квадратная действительная матрица и $S^\\top = -S$. Покажите, что матрица $I - S$ обратима и матрица $(I-S)^{-1}(I + S)$ ортогональна. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2 (25 pts)\n",
    "\n",
    "Рассмотрим языковую модель, обученную для генерации кода на Python. Для удобства скачаем её с Huggingface с помощью кода ниже и выведем, из каких слоёв эта модель состоит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что в модели достаточно много линейных слоёв, которые оперируют с матрицами относительно высокой размерности.\n",
    "Ваша задача состоит в том, чтобы \n",
    "- (15 pts) создать новую модель, в которой линейные слои будут использовать сжатые через SVD линейные слои исходной модели (сжимать аддитивные члены не нужно), итоговая модель должна\n",
    "    - хранить меньше параметров\n",
    "    - быстрее генерировать новый код\n",
    "\n",
    "Постройте график для 5-10 разумных рангов, на которых покажите зависимость времени и памяти от ранга сжатия.\n",
    "\n",
    "- (10 pts) покажите, как сжатие линейных слоёв влияет на качество генерируемого кода. Придумайте 3-5 тестовых строки, которые бы требовали достаточно разнообразного продолжения и проанализируйте какой коэффициент сжатия является критическим для качества генерации.\n",
    "\n",
    "- (bonus! 20 pts) добавьте к сжатию линейных слоёв ещё и сжатие матриц в Attention-слоях. Проведите аналогичное исследование и сделайте вывод о том, какая стратегия сжатия на основе малоранговой аппроксимации матриц наиболее предпочтительна для выбранной модели генераци кода. Для таких моделей генерации должна стать ещё быстрее и число параметров также должно ещё сильнее снизиться.\n",
    "\n",
    "\n",
    "Как генерировать код по первым символам показано в примере ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"def hello_world(x):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output_seq = model.generate(**tokenizer(test_string, return_tensors=\"pt\"), max_length=40,\n",
    "                               num_return_sequences = 5, num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, beam_output in enumerate(beam_output_seq):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
